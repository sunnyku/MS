





# importing the requisite libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

import warnings
warnings.filterwarnings('ignore')









# Reading the dataset

df = pd.read_csv('https://cdn.upgrad.com/UpGrad/temp/098bb2e9-83fb-48df-9dd1-fac56fbb25ca/advertising.csv')


df.info()


data = df[["TV", "Sales"]]
data.head()


# Plotting a scatter plot

sns.scatterplot( data = data , x = 'TV' , y = 'Sales')





# Splitting the dataset into X and y

X = np.array(data['TV']).reshape(-1,1) # predictor variable
y = np.array(data['Sales']).reshape(-1,1) # response variable


# Building the regression model
reg = LinearRegression()
reg.fit(X,y)


# Predictions on the basis of the model
y_pred = reg.predict(X)
# y_pred


# Find the value of r squared
r2_score(y , y_pred)
# The advertising spends on TV explain about 81.21% of the variation in the Sales


# Visualizing the model fit
plt.scatter( X , y , color = 'blue')
plt.plot(X , y_pred , color = 'red' , linewidth = 3)
plt.xlabel("TV (Million $)")
plt.ylabel("Sales (Million $)")
plt.show()


# Model Coefficients: beta0 and beta1
print(reg.intercept_)
print(reg.coef_)


# Metrics to assess model performance 
rss = np.sum(np.square(y - y_pred)) # sum of the squared difference between the actual and the predicted values
print(rss)
mse = mean_squared_error(y, y_pred) # MSE is RSS divided by the number of observations
print(mse)
rmse = mse**0.5 # RMSE is square root of MSE 
print(rmse)





# Residual analysis
y_res = y - y_pred # Residuals


# Residual v/s predictions plot
data['res'] = y_res
plt.scatter( y_pred , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("TV Spend (Million $)")
plt.ylabel("Residual")
plt.show()


# Distribution of errors
p = sns.distplot(y_res,kde=True)

p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residuals")
plt.show()
































# Computing X and Y
X = data['TV'].values # advertising spend on TV
Y = data['Sales'].values # Sales


# Mean X and Y
mean_x = np.mean(X)
mean_y = np.mean(Y)
 
# Total number of values
n = len(X)


# Using the formula to calculate 'b0' and 'b1'
numer = 0
denom = 0
for i in range(n): # for each observation in the data
  numer += (X[i] - mean_x) * (Y[i] - mean_y) # compute the expression and sum over all observations using for loop
  denom += (X[i] - mean_x) ** 2
b1 = numer / denom
b0 = mean_y - (b1 * mean_x)
 
# Printing coefficients
print("Coefficients")
print(b0, b1)





# We use NumPy’s vstack to create a 2-d numpy array from two 1d-arrays and create X_mat.
X_mat=np.vstack((np.ones(len(X)), X)).T


X_mat


Y





# We can implement this using NumPy’s linalg module’s matrix inverse function and matrix multiplication function.
beta_hat = np.linalg.inv(X_mat.T.dot(X_mat)).dot(X_mat.T).dot(Y)


beta_hat


























# Reading the dataset
df = pd.read_csv('https://cdn.upgrad.com/UpGrad/temp/098bb2e9-83fb-48df-9dd1-fac56fbb25ca/advertising.csv')


df.info()


# Inspecting the dataset
print(df.head())


data  = df


# Plotting a scatter plot

sns.scatterplot( data = data , x = 'TV' , y = 'Sales') # advertising spend on TV vs Sales


# Plotting a scatter plot

sns.scatterplot( data = data , x = 'Radio' , y = 'Sales') # Advertising spend on radio versus Sales


# Plotting a scatter plot

sns.scatterplot( data = data , x = 'Newspaper' , y = 'Sales') # Advertising spend on Newspaper versus Sales


# Linear Regression model


# Splitting the dataset into X and y
X = data[['TV' , 'Newspaper', 'Radio']]
y = np.array(data['Sales']).reshape(-1,1)


# Building the regression model
reg = LinearRegression()
reg.fit(X,y)


# Predictions on the basis of the model
y_pred = reg.predict(X)
# y_pred


# Find the value of r squared
r2_score(y , y_pred) # The three predictors considered explain about 90.25% of the variation in the data


# Visual comparison between predicted and actual values
plt.scatter( X['Radio'] , y , color = 'blue') # actual values
plt.scatter( X['Radio'] , y_pred , color = 'red' ) # predicted values
plt.xlabel("Radio (Million $)")
plt.ylabel("Sales (Million $)")
plt.show()


plt.scatter( X['TV'] , y , color = 'blue') # actual values
plt.scatter( X['TV'] , y_pred , color = 'red' ) # predicted values
plt.xlabel("TV (Million $)")
plt.ylabel("Sales (Million $)")
plt.show()


plt.scatter( X['Newspaper'] , y , color = 'blue') # actual values
plt.scatter( X['Newspaper'] , y_pred , color = 'red' ) # predicted values
plt.xlabel("TV (Million $)")
plt.ylabel("Newspaper (Million $)")
plt.show()


# Calculate beta coefficients.
print(reg.intercept_)
print(reg.coef_)


# Metrics to give an overall sense of error in the model
rss = np.sum(np.square(y - y_pred))
print(rss)
mse = mean_squared_error(y, y_pred)
print(mse)
rmse = mse**0.5
print(rmse)


# Residual analysis
y_res = y - y_pred
#y_res


data['res'] = y_res
plt.scatter( y_pred , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Predictions")
plt.ylabel("Residual")
plt.show()


# Distribution of errors
p = sns.distplot(y_res,kde=True)

p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residuals")
plt.show()
































# We use NumPy’s vstack to create a 2-d numpy array from two 1d-arrays and create X_mat.
X_mat=np.vstack((np.ones(len(X)), X.T)).T


#X_mat





# We can implement this using NumPy’s linalg module’s matrix inverse function and matrix multiplication function.
beta_hat = np.linalg.inv(X_mat.T.dot(X_mat)).dot(X_mat.T).dot(y)


beta_hat


beta_hat_list = [beta_hat[i][0] for i in range(len(beta_hat))]
coefficients = ['b0', 'b1(TV)', 'b2(Radio)', 'b3(Newspaper)']
betas = dict(zip(coefficients, beta_hat_list))


betas
































dist = pd.read_csv(r"AR - Examples - 1.5.csv")
dist.head()


# Plotting a scatter plot
sns.scatterplot( data = dist , x ='time' , y='distance')
plt.show()


# Splitting the dataset into X and y
X = np.array(dist['time']).reshape(-1,1)
y = np.array(dist['distance']).reshape(-1,1)


# Building the regression model
model = LinearRegression()


model.fit(X, y)


# Predictions on the basis of the model
y_pred2 = model.predict(X)
# y_pred2


# Find the value of r squared
r2_score(y, y_pred2)


plt.scatter( X , y , color = 'blue')
plt.plot(X , y_pred2 , color = 'red' , linewidth = 3)
plt.xlabel("time")
plt.ylabel("distance")
plt.show()


# Calculate beta0 and beta1.
print(model.intercept_)
print(model.coef_)


# Metrics to give an overall sense of error in the model
rss = np.sum(np.square(y - y_pred2))
print(rss)
mse = mean_squared_error(y, y_pred2)
print(mse)
rmse = mse**0.5
print(rmse)



#residual
residual = y - y_pred2


# Scatter plot of the predicted values on the x-axis and the residuals on the y-axis
plt.scatter( y_pred2 , residual)
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Predicted Distance (metres)")
plt.ylabel("Residual")
plt.show()


# Distribution of errors
p = sns.distplot(residual,kde=True)
p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residual")
plt.show()


# As we can see that the residuals do not fulfill the conditions for linear regression, Lets see if we can make some changes so that the residuals are normally distributed.

dist['time (seconds)(log)'] = np.log(dist['time'])


# Plotting a scatter plot
sns.scatterplot( data = dist , x ='time (seconds)(log)' , y='distance')
plt.show()


# Splitting the dataset into X and y
X = np.array(dist['time (seconds)(log)']).reshape(-1,1)
y = np.array(dist['distance']).reshape(-1,1)


# Building the regression model
model = LinearRegression()

model.fit(X, y)


# Predictions on the basis of the model
y_pred2 = model.predict(X)
# y_pred2


# Find the value of r squared
r2_score(y, y_pred2)


plt.scatter( X , y , color = 'blue')
plt.plot(X , y_pred2 , color = 'red' , linewidth = 3)
plt.xlabel("time (seconds)(log)")
plt.ylabel("distance")
plt.show()


# Calculate beta0 and beta1.
print(model.intercept_)
print(model.coef_)


# Metrics to give an overall sense of error in the model
rss = np.sum(np.square(y - y_pred2))
print(rss)
mse = mean_squared_error(y, y_pred2)
print(mse)
rmse = mse**0.5
print(rmse)


#residual
residual = y - y_pred2


plt.scatter( y_pred2 , residual)
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Predicted Distance (metres)")
plt.ylabel("Residual")
plt.show()



# Distribution of errors
p = sns.distplot(residual,kde=True)
p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residual")
plt.show()





# Model to predict marks given the number of courses taken and the time the student gives to study 
# on a daily basis.
data = pd.read_csv("AR - Examples - 1.6.csv")
data.head()


# Plotting a scatter plot
sns.scatterplot( data = data , x = 'number_courses' , y = 'Marks')


# Plotting a scatter plot
sns.scatterplot( data = data , x = 'time_study' , y = 'Marks')


# Splitting the dataset into X and y
X = data[['number_courses' , 'time_study']]
y = np.array(data['Marks']).reshape(-1,1)


# Building the regression model
reg = LinearRegression()
reg.fit(X,y)


# Predictions on the basis of the model
y_pred = reg.predict(X)
#y_pred


# Find the value of r squared
r2_score(y , y_pred)


# Calculate beta0 and beta1.
print(reg.intercept_)
print(reg.coef_)


# Metrics to give an overall sense of error in the model
rss = np.sum(np.square(y - y_pred))
print(rss)
mse = mean_squared_error(y, y_pred)
print(mse)
rmse = mse**0.5
print(rmse)


# Residual analysis
y_res = y - y_pred
#y_res


data['res'] = y_res
plt.scatter( y_pred , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Predictions")
plt.ylabel("Residual")
plt.show()





data['res'] = y_res
plt.scatter( data['number_courses'] , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Number courses")
plt.ylabel("Residual")
plt.show()


plt.scatter( data['time_study'] , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Time study")
plt.ylabel("Residual")
plt.show()


# Distribution of errors
p = sns.distplot(y_res,kde=True)

p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residuals")
plt.show()






































# Transforming the time_study variable
data['time_study_squared'] = data['time_study']*data['time_study']


plt.scatter( data['time_study_squared'] , data['Marks'])
plt.xlabel("Time study squared")
plt.ylabel("Marks")
plt.show()


# Splitting the dataset into X and y
X = data[['number_courses' , 'time_study', 'time_study_squared']] 
y = np.array(data['Marks']).reshape(-1,1)


# Building the regression model
reg = LinearRegression()
reg.fit(X,y)


# Predictions on the basis of the model
y_pred = reg.predict(X)
# y_pred


# Find the value of r squared
r2_score(y , y_pred)


plt.scatter( X['number_courses'] , y , color = 'red')
plt.scatter( X['number_courses'] , y_pred , color = 'blue' )
plt.xlabel("Number Courses")
plt.ylabel("Marks")
plt.show()


plt.scatter( X['time_study_squared'] , y , color = 'red')
plt.scatter( X['time_study_squared'] , y_pred , color = 'blue' )
plt.xlabel("Time Study Squared")
plt.ylabel("Marks")
plt.show()


# Calculate beta0 and beta1.
print(reg.intercept_)
print(reg.coef_)


#Residual Sum of Squares = Mean_Squared_Error * Total number of datapoints
rss = np.sum(np.square(y - y_pred))
print(rss)
mse = mean_squared_error(y, y_pred)
print(mse)
rmse = mse**0.5
print(rmse)


# Residual analysis
y_res = y - y_pred
#y_res


data['res'] = y_res
plt.scatter( y_pred , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Predictions")
plt.ylabel("Residual")
plt.show()


data['res'] = y_res
plt.scatter( data['number_courses'] , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Number courses")
plt.ylabel("Residual")
plt.show()


plt.scatter( data['time_study_squared'] , data['res'])
plt.axhline(y=0, color='r', linestyle=':')
plt.xlabel("Time study Squared")
plt.ylabel("Residual")
plt.show()


# Distribution of errors
p = sns.distplot(y_res,kde=True)

p = plt.title('Normality of error terms/residuals')
plt.xlabel("Residuals")
plt.show()



