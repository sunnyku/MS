X and Y should display some sort of a linear relationship; otherwise, there is no use of fitting a linear model between them.



Error terms are normally distributed with mean zero(not X, Y):

    There is no problem if the error terms are not normally distributed if you just wish to fit a line and not make any further interpretations.
    But if you are willing to make some inferences on the model that you have built (you will see this in the coming segments), you need to have a notion of the distribution of the error terms. One particular repercussion of the error terms not being normally distributed is that the p-values obtained during the hypothesis test to determine the significance of the coefficients become unreliable. (You'll see this in a later segment)
    The assumption of normality is made, as it has been observed that the error terms generally follow a normal distribution with mean equal to zero in most cases.


Yes! Even if you fit a line through the data, you cannot make inferences about the model. The parameters used to make inferences (which you will study in later segments) will become highly unreliable.


Correct! As is evident from the graph, the error terms seem to be reducing with an increase in the value of X. This is clearly a violation of the assumption that the error terms have constant variance.


You start by saying that β1 is not significant, i.e. there is no relationship between X and y.

So in order to perform the hypothesis test, we first propose the null hypothesis that β1 is 0. And the alternative hypothesis thus becomes β1 is not zero.

    Null Hypothesis (H0): β1=0
    Alternate Hypothesis (HA): β1≠0

Now, how do you perform the hypothesis test? Recall from your hypothesis testing module that you first used to compute the t-score (which is very similar to the Z-score) which is given by X−μs/√n where μ is the population mean and s is the sample standard deviation which when divided by √n is also known as standard error.



Using this, the t-score for ^β1 comes out to be (since the null hypothesis is that β1 is equal to zero):



^β1−0SE(^β1)



Now, in order to perform the hypothesis test, you need to derive the p-value for the given beta. If you're hazy on what p-value is and how it is calculated, it is recommended that you revisit the segment on p-value. Please note that the formula of SE(β1) provided in the t-score above is out of scope of this course.



Let's do a quick recap of how do you calculate p-value anyway:

    Calculate the value of t-score for the mean point (in this case, zero, according to the Null hypothesis that we have stated) on the distribution
    Calculate the p-value from the cumulative probability for the given t-score using the t-table
    Make the decision on the basis of the p-value with respect to the given value of β  (significance level)



Now, if the p-value turns out to be less than 0.05, you can reject the null hypothesis and state that β1 is indeed significant.

Feedback:

Correct! The Null Hypothesis in simple linear regression is:

β1=0

Thus, if we fail to reject the Null hypothesis, it means that β1 is indeed zero, and thus insignificant for the prediction of the independent variable.



The t-statistic along with the t-distribution table is used to determine the p-value of the coefficient.


Correct! In case of a small sample size, we use a t-distribution which is very similar to a normal distribution.


Recall that the t-score for β1 is given as β1SE(β1).

Hence, you have:

t-score = 0.50.02=25


Correct! Recall that a t-distribution is very similar to a normal distribution. And a value as big as 25 means a practically zero p-value which in turn means that the variable is significant. You can have a look at the t-table here anyway. And you'll anyway see this in the Python demo in the next segment.


Yes! You can view both the parameters using this simple command.

lr .summary()
✓ Correct
Feedback:

The summary() function also outputs the values of coefficients and hence, can be used to view these values as well.Feedback:

Correct! If you look at the summary statistics, you can see that the F-statistic has a value of 270.2 which is a very high value and this, the Prob(F-statistic) is 5.93e-56 (as shown in the table) which is a practically zero value. Hence, the value of less than 0.05 which means that the overall model fit is significant.

Feedback:

Correct! If you look at the table, you can see that the p-value for the coefficient of the variable 'Frequency' is 0 which is a low value and hence, the coefficient is significant.
The R-squared value is low and hence, the model doesn't explain much of the variance.
✓ Correct
Feedback:

Correct! Look at the summary statistics closely. The value of R-squared is 0.153. Recall that R-squared varies fromYes! Recall that RMSE (Root Mean Squared Error) is a metric that tells you the deviation of the predicted values by a model from the actual observed values. So, since it is a sort of error term, it is better to have a low RMSE.

Notice that the RMSE for the first model is lesser than the second model. So naturally, this model would be better than the other. 0 to 1 wherein a value of 0 implies that none of the variance in the data is explained and a value of 1 implies that all of the variance in the data is explained. Can you answer the question now? Hence, a value of 0.153 is a low value of R-squared which in turn implies that the model doesn't explain much variance present in the data.

Feedback:

Correct! lm.coeff_ gives you the value of β1 which is the slope of the fitted line.


    A quick recap of simple linear regression
    Assumptions of simple linear regression
        Linear relationship between X and y.
        Normal distribution of error terms.
        Independence of error terms.
        Constant variance of error terms.
    Hypothesis testing in linear regression
        To determine the significance of beta coefficients.
        H0:β1=0;HA:β1≠0.
        T-test on the beta coefficient.
        t score=^βiSE(^βi).
    Building a linear model
        OLS (Ordinary Least Squares) method in statsmodels to fit a line.
        Summary statistics
            F-statistic, R-squared, coefficients and their p-values.
    Residual Analysis
        Histogram of the error terms to check normality.
        Plot of the error terms with X or y to check independence.
    Predictions
        Making predictions on the test set using the 'predict()' function.
    Linear Regression using SKLearn
        A second package apart from statsmodels for linear regression.
        A more hassle-free package to just fit a line without any inferences.

Correct! The least square error which gives the sum of the square of differences between the actual values and the predicted values (using the regression line fitted) is used to determine the best fit line. The key to getting the best fit line is minimising these errors.

Which of the following step(s) are required to fit a straight line through a set of data points using statsmodels.api? Multiple options can be correct.

sm.add_constant(X)
✓ Correct
Feedback:

Correct! The two simple steps involved to fit a line are:

sm.add_constant(X)

sm.OLS(y, X).fit()

sm.OLS(y, X).fit()
✓ Correct
Feedback:

Correct! The two simple steps involved to fit a line are:

sm.add_constant(X)

sm.OLS(y, X).fit()
Feedback:

The Null hypothesis in the case of linear regression is:

βi=0

So if your p-value is less than 0.05, you can reject the Null Hypothesis and conclude that the coefficient is significant.

Also, note that 0.05 is just a conventional cutoff. Based on your requirement, you can set the cutoff to anything; it might be a higher value like 0.1 or a lower value like 0.02.

Correct! Recall that one of the assumptions of linear regression was, the residuals are normally distributed around zero, i.e. their mean is equal to zero. Hence, the sum of residuals should also be zero.


Feedback:

Correct! In order to determine the overall model fit, the F-statistic is used.

The basic idea behind the F-test is that it is a relative comparison between the model that you've built and the model without any of the coefficients except for β0 . If the value of the F-statistic is high, it would mean that the Prob(F) would be low and hence, you can conclude that the model is significant. On the other hand, if the value of F-statistic is low, it might lead to the value of Prob(F) being higher than the significance level (taken 0.05, usually) which in turn would conclude that the overall model fit is insignificant and the intercept-only model can provide a better fit.



t=βiSE(βi)
✓ Correct
Feedback:

Correct! The t-value for a particular coefficient is given by the coefficient divided by its standard error.
