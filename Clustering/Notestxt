PRACTICAL APPLICATIONS OF CLUSTERING

 

Customer Insight: Say, a retail chain with so many stores across locations wants to manage stores at best and increase the sales and performance. Cluster analysis can help the retail chain to get desired insights on customer demographics, purchase behaviour and demand patterns across locations. This will help the retail chain for assortment planning, planning promotional activities and store benchmarking for better performance and higher returns.
Marketing: Cluster Analysis can help with In the field of marketing, Cluster Analysis can help in market segmentation and positioning, and to identify test markets for new product development.
Social Media: In the areas of social networking and social media, Cluster Analysis is used to identify similar communities within larger groups.
Medical: Cluster Analysis has also been widely used in the field of biology and medical science like human genetic clustering, sequencing into gene families, building groups of genes, and clustering of organisms at species. 



In clustering, we group the data points into different categories based on the given set of attributes. There are no dependent and independent variables


You saw that mainly 3 types of segmentation are used for customer segmentation:

Behavioural segmentation: Segmentation is based on the actual patterns displayed by the consumer
Attitudinal segmentation: Segmentation is based on the beliefs or the intents of people, which may not translate into similar action
Demographic segmentation: Segmentation is based on the person’s profile and uses information such as age, gender, residence locality, income, etc.
 
 
 Behavioral segmentation
✓ Correct
Feedback:
Recharge is a behaviour which can be observed, opposed to attitude which resides in the mindset of the customer.
 
 You are doing a demographic segmentation, since you are looking at the income and the profession of people. Notice how this is much simpler than finding data about actual laptop purchasing history of customers and then trying to estimate the market size based on that.


Clustering is a technique in data science and machine learning that groups a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups.

Applications of Clustering:
Looking at social media behaviour to find out what types of online communities are there:














In simple terms, the algorithm needs to find data points whose values are similar to each other and therefore these points would then belong to the same cluster. The method in which any clustering algorithm goes about doing that is through the method of finding something called a “distance measure”. The distance measure that is used in K-means clustering is called the Euclidean Distance measure. Let’s look at the following lecture to understand how this value is calculated.

As mentioned in the video above, the Euclidean Distance between the 2 points is measured as follows: If there are 2 points X and Y having n dimensions

 

X
=
(
X
1
,
X
2
,
X
3
,
.
.
.
X
n
)

Y
=
(
Y
1
,
Y
2
,
Y
3
,
.
.
.
.
Y
n
)

 

Then the Euclidean Distance D is given as 

 

D
=
√
(
X
1
−
Y
1
)
2
+
(
X
2
−
Y
2
)
2
+
.
.
.
(
X
n
−
Y
n
)
2

 

The idea of distance measure is quite intuitive. Essentially, the observations which are closer or more similar to each other would have a low Euclidean distance and the observations which are farther or less similar to each other would have a higher Euclidean distance. So can you now guess how the Clustering process would work based on the Euclidean distance?

 
 If you remember your high school geometry, centroids are essentially the centre points of triangles. Similarly, in the case of clustering, centroids are the centre points of the clusters that are being formed.
 

the Centroids are essentially the cluster centres of a group of observations that help us in summarising the cluster's properties. 
rr
 
 



Clustering can group users based on their behavior, interests, or interactions, which helps in understanding the types of communities that exist online.
Identify consumer segments and their properties to position products appropriately:

Clustering can be used to identify groups of customers with similar purchasing behaviors, preferences, and demographics, which helps businesses tailor their marketing strategies and product positioning.
Identifying patterns of crime in different regions of a city and managing police enforcement based on frequency and type of crime:

Clustering can help analyze crime data to detect patterns, identify crime hotspots, and allocate resources more effectively.

SQRT(((Xi-xi)^2) + (Yi-yi)^2))
✓ Correct
Feedback:
This is the formula for the Euclidean distance. You can see that once the cells S6:T16 are filled with the required distances, the points are assigned to one of the clusters (marked in column E) based on the minimum distance.

Calculate the centroid of the points assigned to a particular cluster in the previous step
✓ Correct
Feedback:
The way is new center is calculated in K-Mean clustering is the mean of all the data points belonging to that cluster.Notice that the new centers are already filled in the excel sheet.

The K-Means algorithm uses the concept of the centroid to create K clusters. Before you move ahead, it will be useful to recall the concept of the centroid.

 

In simple terms, a centroid of n points on an x-y plane is another point having its own x and y coordinates and is often referred to as the geometric centre of the n points.

 

For example, consider three points having coordinates (x1, y1), (x2, y2) and (x3, y3). The centroid of these three points is the average of the x and y coordinates of the three points, i.e.

(x1 + x2 + x3 / 3, y1 + y2 + y3 / 3).

 

Similarly, if you have n points, the formula (coordinates) of the centroid will be:

(x1+x2…..+xn / n, y1+y2…..+yn / n). 

 

So let’s see how the K-Means algorithm achieves this goal.
From the previous lecture, we understood that the algorithm’s inner-loop iterates over two steps:

Assign each observation 
X
i
 to the closest cluster centroid 
μ
k
Update each centroid to the mean of the points assigned to it.
In the assignment step, we assign every data point to K clusters. The algorithm goes through each of the data points and depending on which cluster is closer, in our case, whether the green cluster centroid or the blue cluster centroid; It assigns the data points to one of the 2 cluster centroids.

 

The equation for the assignment step is as follows:

 

Z
i
=
a
r
g
m
i
n
|
|
X
i
−
μ
k
|
|
2

 

Now having assigned each data point to a cluster, now we need to recompute the cluster centroids. In the next lecture, Prof.Dinesh will explain how to recompute the cluster centroids or the mean of each cluster.

In the optimisation step, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.

"argmin" is a tool for finding the optimal solution in situations where we want to minimize a particular function.
 

The equation for optimisation is as follows:

 

μ
k
=
1
n
k
∑
i
:
z
i
=
k
X
i

 

The process of assignment and optimisation is repeated until there is no change in the clusters or possibly until the algorithm converges.

For a 
i
t
h
 data point which is a 2d object and 
μ
 which is again a 2d object, we compute the distance between these two, this is given by 
d
(
x
i
,
μ
k
)
 where k is the number of clusters and then from these k different results we will choose the minimum of all.
 
 
 Clustering Players in Sports

Clustering is a powerful technique used in sports analytics to group players based on similar characteristics or behaviors. By analyzing player data, such as statistics, playing style, and physical attributes, clustering algorithms can identify distinct groups or "archetypes" within a player population.

Common applications of clustering in sports include:

Player profiling: Identifying different player types, such as "scorers," "playmakers," or "defenders," to better understand their roles and contributions.
Team building: Identifying complementary players to create balanced and effective teams.
Player scouting: Identifying potential recruits who fit a team's specific needs or playing style.
Injury prevention: Identifying players at risk of injury based on their physical characteristics and playing style.
Performance analysis: Evaluating player performance against their respective clusters to identify strengths and weaknesses.
Clustering algorithms like K-means, hierarchical clustering, and DBSCAN are commonly used for player clustering. By effectively grouping players, teams can make data-driven decisions to improve performance and achieve their goals.









To summarise, In K-Means++ algorithm,

We choose one center as one of the data points at random.
For each data point 
X
i
, We compute the distance between 
X
i
 and the nearest center that had already been chosen.
Now, we choose the next cluster center using the weighted probability distribution where a point 
X
 is chosen with probability proportional to 
d
(
X
)
2
 .
Repeat Steps 2 and 3 until 
K
 centers have been chosen.

We looked in the previous segment that for K-Means optimisation problem, the algorithm it iterate between two steps and tries to minimise the objective function given as,

 

Z
i
=
a
r
g
m
i
n
|
|
X
i
−
μ
k
|
|
2

 

 

To choose the cluster centers smartly, we will learn about K-Mean++ algorithm. K-means++ is just an initialisation procedure for K-means. In K-means++ you pick the initial centroids using an algorithm that tries to initialise centroids that are far apart from each other.
Intuitively, it looks like 2 clusters are present. If we use K means, then we will get wrong clusters since the points in the outer ring like structure will not be segmented accurately. A reason for that is K-Means looks for how close the points are to a centroid and this distance or measure of closeness is the “linear distance”. One way to correct this can be to see the distance between all the points and then cluster the closest points.



the major practical considerations involved in K-Means clustering are:

The number of clusters that you want to divide your data points into, i.e. the value of K has to be pre-determined.

The choice of the initial cluster centres can have an impact on the final cluster formation.

The clustering process is very sensitive to the presence of outliers in the data.

Since the distance metric used in the clustering process is the Euclidean distance, you need to bring all your attributes on the same scale. This can be achieved through standardisation.

The K-Means algorithm does not work with categorical data.

The process may not converge in the given number of iterations. You should always check for convergence.



So to compute silhouette metric, we need to compute two measures i.e. 
a
(
i
)
 and 
b
(
i
)
 where,

a
(
i
)
 is the average distance from own cluster(Cohesion).
b
(
i
)
 is the average distance from the nearest neighbour cluster(Separation). 
Now, let's look at how to combine cohesion and separation to compute the silhouette metric.

Since each run of K-means is independent, multiple runs can find different local optima, and this can help in choosing the global optimum value.

 it's important to check whether the given data has some meaningful clusters or not? which in general means the given data is not random. The process to evaluate the data to check if the data is feasible for clustering or not is know as the clustering tendency.

 

As we have already discussed in the previous lecture that the clustering algorithm will return K clusters even if that data does not have any clusters or have any meaningful clusters. So before proceeding for clustering, we should not blindly apply the clustering method and we should check the clustering tendency.

The algorithm begins with choosing K random cluster centres.

 

Then the 2 steps of Assignment and Optimisation continue iteratively till the clusters stop updating. This gives you the most optimal clusters — the clusters with minimum intra-cluster distance and maximum inter-cluster distance.

 

You also saw the different practical issues that need to be considered while employing clustering to your data set. You need to choose how many clusters you want to group your data points into. Secondly, the K-means algorithm is non-deterministic. This means that the final outcome of clustering can be different each time the algorithm is run even on the same data set. This is because, as you saw, the final cluster that you get can vary by the choice of the initial cluster centres.

 

You also saw that the outliers have an impact on the clusters and thus outlier-infested data may not give you the most optimal clusters. Similarly, since the most common measure of the distance is the Euclidean distance, you would need to bring all the attributes into the same scale using standardisation.

 

You also saw that you cannot use categorical data for the K-Means algorithm. There are other customised algorithms for such categorical data.

K-Means algorithm
Arrange the steps of k-means algorithm in the order in which they occur:

Randomly selecting the cluster centroids
Updating the cluster centroids iteratively
Assigning the cluster points to their nearest center

1-2-3


1-3-2

✓ Correct
Feedback:
First the cluster centers are pre-decided. Then all the points are assigned to their nearest cluster center and then the center is recalculated as the mean of all the points which fall in that cluster. Then the clustering is repeated with the new centers and the centers are updated according to the new cluster points.

A

✓ Correct
Feedback:
According to k-means algorithm, the point should be assigned to the centre with the minimum distance from the point. The distances for A, B, C are sqrt(2), sqrt(18) and sqrt(25)

Which of the following options are prerequisites for k-means algorithm:

 

A) initial centers should be very close to each other

B) Choice of number of clusters

C) Choice of initial centroids

￼
Only A

￼
B & C

✓ Correct
Feedback:
Note that the k-means algorithm requires the initial centers to be far apart.

Hopkins Statistics

 

One more important data preparation technique that we also need to do but have skipped in the demonstration is the calculation of the Hopkins Statistic. In python, you can use the following code snippet to pass a dataframe to the Hopkins statistic function to find if the dataset is suitable for clustering or not. You can simply copy-paste the code present in the code given below to the main dataset and analyse the Hopkins statistic value.


Notes regarding Hopkins Statistic

You don't need to know how the algorithm of  Hopkins Statistic works. The algorithm is pretty advanced and hence you don't need to know its workings but rather only interpret the value that it assigns to the dataframe.
On multiple iterations of Hopkins Statistic, you would be getting multiple values since the algorithm uses some randomisation in the initialisation part of the code. Therefore it is advised to run it a couple of times before confirming whether the data is suitable for clustering or not.

One really easy way is to see if the .are logically correct. For example, we can check if states in a similar geography and economic situation are clustered together or not. You can also run hypothesis test to check whether the population of different clusters is significantly different or not, If you look at the data, you can see that some specific customers or some specific states should be grouped together.

Feedback:
Look at the clusters formed with and without scaling. You will see that for the ones formed without scaling, the states with similar literacy rates will fall in the same cluster, even though their graduate percentage differs.


One of the major considerations in using the K-means algorithm is deciding the value of K beforehand. The hierarchical clustering algorithm does not have this restriction.

 

The output of the hierarchical clustering algorithm is quite different from the K-mean algorithm as well. It results in an inverted tree-shaped structure, called the dendrogram. An example of a dendrogram is shown below.



In the K-Means algorithm, you divided the data in the first step itself. In the subsequent steps, you refined our clusters to get the most optimal grouping. In hierarchical clustering, the data is not partitioned into a particular cluster in a single step. Instead, a series of partitions/merges take place, which may run from a single cluster containing all objects to n clusters that each contain a single object or vice-versa.

 

This is very helpful since you don’t have to specify the number of clusters beforehand.

 

Given a set of N items to be clustered, the steps in hierarchical clustering are:

Calculate the NxN distance (similarity) matrix, which calculates the distance of each data point from the other

Each item is first assigned to its own cluster, i.e. N clusters are formed

The clusters which are closest to each other are merged to form a single cluster

The same step of computing the distance and merging the closest clusters is repeated till all the points become part of a single cluster

 

Thus, what you have at the end is the dendrogram, which shows you which data points group together in which cluster at what distance. You will learn more about interpreting the dendrogram in the next segment.

 
 The result of the cluster analysis is shown by a dendrogram, which starts with all the data points as separate cluster and indicates at what level of dissimilarity any two clusters were joined.
 
 
 In the dendrogram shown above, samples 4 and 5 are the most similar and join to form the first cluster, followed by samples 1 and 10. The last two clusters to fuse together to form the final single cluster are 3-6 and 4-5-2-7-1-10-9-8. 

 

Determining the number of groups in a cluster analysis is often the primary goal. Typically, one looks for natural groupings defined by long stems. Here, by observation, you can identify that there are 3 major groupings: 3-6, 4-5-2-7 and 1-10-9-8.

 

You also saw that hierarchical clustering can proceed in 2 ways — agglomerative and divisive. If you start with n distinct clusters and iteratively reach to a point where you have only 1 cluster in the end, it is called agglomerative clustering. On the other hand, if you start with 1 big cluster and subsequently keep on partitioning this cluster to reach n clusters, each containing 1 element, it is called divisive clustering.


Let’s see once again the different types of linkages.

Single Linkage: Here, the distance between 2 clusters is defined as the shortest distance between points in the two clusters
Complete Linkage: Here, the distance between 2 clusters is defined as the maximum distance between any 2 points in the clusters
Average Linkage: Here, the distance between 2 clusters is defined as the average distance between every point of one cluster to every other point of the other cluster.
 

You have to decide what type of linkage should be used by looking at the data. One convenient way to decide is to look at how the dendrogram looks. Usually, single linkage type will produce dendrograms which are not structured properly, whereas complete or average linkage will produce clusters which have a proper tree-like structure. You will see later what this means when you run the hierarchical clustering algorithm in Python.


 In complete linkage hierarchical clustering, the inter cluster distance is defined as the longest distance between two points (one point in each cluster)
✓ Correct
Feedback:
In the complete linkage, inter cluster distance is calculated as the maximum distance between 2 points (one in each cluster), However, the point is assigned to a new cluster basis it’s minimum distance from the clusters
 
 So, you learnt that whether you use k-means or hierarchical clustering algorithm depends on your hardware and the data that you are dealing with.

 

Now, you will look at a good statistical hack to solve segmentation problems so that you get meaningful segments, where you will use both hierarchical and k-means algorithms to complement each other.


Hierarchical clustering generally produces better clusters, but is more computationally intensive.

Yes, you can use a dendrogram to make meaningful clusters by analyzing the height at which elements leave and join.

Here's how to interpret a dendrogram to identify

Average and Complete linkage methods give a well-separated dendrogram, whereas single linkage gives us dendrograms which are not very well separated. We generally want well separated clusters.

